{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH+fOLTlPj9PLajzKHhuRj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usshaa/Deep-Learning-For-all/blob/main/Module_3_Deep_Neural_Networks_(DNNs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ“Œ Module 3: Deep Neural Networks (DNNs)**  \n",
        "### ğŸ¯ **Learning Objectives**  \n",
        "- Understand the **architecture of Deep Neural Networks (DNNs)**  \n",
        "- Learn about **Hyperparameter Tuning** (Learning Rate, Batch Size, Dropout)  \n",
        "- Explore **Regularization Techniques** (L1, L2, Dropout, Batch Normalization)  \n",
        "- Understand the **Vanishing & Exploding Gradient Problem**  \n",
        "- Learn different **Weight Initialization Techniques**  \n",
        "\n",
        "---\n",
        "\n",
        "## **ğŸ”¹ 1. Architecture of Deep Neural Networks (DNNs)**  \n",
        "A **Deep Neural Network (DNN)** is an extension of a **Feedforward Neural Network (FNN)** with multiple **hidden layers** between input and output.  \n",
        "\n",
        "### âœ… **Typical DNN Architecture**\n",
        "1. **Input Layer**: Takes raw data (e.g., images, text, numerical data).  \n",
        "2. **Hidden Layers**: Multiple layers with neurons applying activation functions.  \n",
        "3. **Output Layer**: Produces the final prediction (e.g., classification, regression).  \n",
        "\n",
        "### âœ… **Code: Build a Simple DNN in TensorFlow**\n"
      ],
      "metadata": {
        "id": "a4ayNzBivTV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "-crN1mRSTUnj"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Deep Neural Network (DNN)\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(10,)),  # Hidden Layer 1\n",
        "    keras.layers.Dense(128, activation='relu'),  # Hidden Layer 2\n",
        "    keras.layers.Dense(64, activation='relu'),  # Hidden Layer 3\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)\n",
        "])"
      ],
      "metadata": {
        "id": "LQk-XQKcvduu"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "6V6vReukvfVg",
        "outputId": "d2d4c7fd-d601-42e1-ba11-0921da77bb8a"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚             \u001b[38;5;34m704\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚           \u001b[38;5;34m8,256\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   â”‚              \u001b[38;5;34m65\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,345\u001b[0m (67.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,345</span> (67.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,345\u001b[0m (67.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,345</span> (67.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… **Explanation:**  \n",
        "- **3 hidden layers** with ReLU activation  \n",
        "- **Output layer** uses **Sigmoid** (for binary classification)  \n",
        "- **Adam optimizer** for better weight updates  \n",
        "\n",
        "---\n",
        "\n",
        "## **ğŸ”¹ 2. Hyperparameter Tuning**  \n",
        "Hyperparameters affect the learning process and performance of the model.\n",
        "\n",
        "| Hyperparameter | Description | Best Practices |\n",
        "|---------------|------------|---------------|\n",
        "| **Learning Rate (LR)** | Controls step size in weight updates | Start with `0.001`, adjust based on performance |\n",
        "| **Batch Size** | Number of samples per training step | Small batch â†’ More noise, Large batch â†’ Faster training |\n",
        "| **Dropout** | Prevents overfitting by randomly deactivating neurons | Typical values: `0.2 - 0.5` |\n",
        "\n",
        "### âœ… **Code: Experimenting with Learning Rate**\n"
      ],
      "metadata": {
        "id": "aNYDcSULvhnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model with Different Learning Rates\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "n11vfel6vg_I"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… **Tip:** If training is slow, **increase learning rate**; if unstable, **decrease learning rate**.\n",
        "\n",
        "---\n",
        "\n",
        "## **ğŸ”¹ 3. Regularization Techniques**  \n",
        "Regularization prevents **overfitting** by reducing model complexity.\n",
        "\n",
        "### âœ… **L1 & L2 Regularization (Lasso & Ridge)**\n",
        "| Regularization | Formula | Effect |\n",
        "|--------------|---------|--------|\n",
        "| **L1 (Lasso)** | \\( L1 = \\lambda \\sum |w| \\) | Encourages sparsity (zero weights) |\n",
        "| **L2 (Ridge)** | \\( L2 = \\lambda \\sum w^2 \\) | Prevents large weight values |\n",
        "\n",
        "âœ… **Example: Adding L1 & L2 Regularization**\n"
      ],
      "metadata": {
        "id": "ZWfRpD6IvmgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', kernel_regularizer=l1(0.01)),  # L1 Regularization\n",
        "    keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # L2 Regularization\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "cdKic9F3vl5W"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **Dropout**\n",
        "- Randomly drops **neurons** during training to prevent reliance on specific features.\n",
        "\n",
        "âœ… **Example: Adding Dropout**\n"
      ],
      "metadata": {
        "id": "tAEyS7JIvsJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),  # Drops 30% of neurons\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "oigKT5xYvrh7"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **Batch Normalization**\n",
        "- Normalizes activations to stabilize learning and speed up convergence.\n",
        "\n",
        "âœ… **Example: Adding Batch Normalization**\n"
      ],
      "metadata": {
        "id": "EsYa2aDkvx9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "R47RyImMvxhy"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ğŸ”¹ 4. Vanishing & Exploding Gradient Problem**  \n",
        "### **âŒ Problem:**\n",
        "- **Vanishing Gradients**: Small gradients â†’ Slow learning â†’ Poor performance.  \n",
        "- **Exploding Gradients**: Large gradients â†’ Unstable updates â†’ Training divergence.  \n",
        "\n",
        "### **âœ… Solutions**\n",
        "| Solution | Description |\n",
        "|----------|------------|\n",
        "| **ReLU Activation** | Prevents vanishing gradient (compared to Sigmoid/Tanh) |\n",
        "| **Batch Normalization** | Keeps activations stable |\n",
        "| **Gradient Clipping** | Limits gradient values to avoid explosion |\n",
        "| **Proper Weight Initialization** | Ensures proper signal propagation |\n",
        "\n",
        "âœ… **Example: Using Gradient Clipping**\n"
      ],
      "metadata": {
        "id": "617jS1aIv5TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.0), loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "Od6o9zm9v41n"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ğŸ”¹ 5. Weight Initialization Techniques**  \n",
        "Proper weight initialization **avoids vanishing or exploding gradients**.\n",
        "\n",
        "| Initialization | Purpose | Formula |\n",
        "|---------------|---------|---------|\n",
        "| **Random Initialization** | Assigns small random values | \\( w \\sim U(-0.5, 0.5) \\) |\n",
        "| **Xavier (Glorot)** | Keeps variance stable across layers | \\( w \\sim \\mathcal{N}(0, \\frac{1}{n}) \\) |\n",
        "| **He Initialization** | Designed for ReLU networks | \\( w \\sim \\mathcal{N}(0, \\frac{2}{n}) \\) |\n",
        "\n",
        "âœ… **Example: Using He Initialization (for ReLU)**\n"
      ],
      "metadata": {
        "id": "Hh8Qjp6Gv-ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', kernel_initializer=HeNormal()),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "dfbk1lbWv-Jb"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ“Š Summary**\n",
        "| Topic | Key Takeaways |\n",
        "|-------|--------------|\n",
        "| **DNN Architecture** | Multiple hidden layers for deeper learning |\n",
        "| **Hyperparameter Tuning** | Learning Rate, Batch Size, Dropout |\n",
        "| **Regularization** | L1/L2 (weight penalties), Dropout, Batch Normalization |\n",
        "| **Gradient Problems** | Vanishing (slow learning), Exploding (unstable updates) |\n",
        "| **Weight Initialization** | He (ReLU), Xavier (Sigmoid/Tanh) |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "minUUhgpwEvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79-alal0wENS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}